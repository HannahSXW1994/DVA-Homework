Section A 
1.weka.classifiers.trees.RandomForest -P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1Time taken to build model: 0.31 seconds
Overall accuracy: 86.6769%Confusion   Matrix:   a   b   <-- classified as
 316  41 |   a = 0
  46 250 |   b = 1

2.
weka.classifiers.functions.Logistic -R 1.0E-8 -M -1 -num-decimal-places 4
Time taken to build model: 0.16 seconds
Overall accuracy: 86.3706%
Confusion   Matrix:
   a   b   <-- classified as
 302  55 |   a = 0
  34 262 |   b = 1

3.
weka.classifiers.functions.MultilayerPerceptron -L 0.3 -M 0.2 -N 500 -V 0 -S 0 -E 20 -H a
Time taken to build model: 5.33 seconds
Overall accuracy: 83.3078%
Confusion   Matrix:
   a   b   <-- classified as
 298  59 |   a = 0
  50 246 |   b = 1

4.
weka.classifiers.functions.SGD -F 0 -L 0.01 -R 1.0E-4 -E 500 -C 0.001 -S 1
Time taken to build model: 0.09 seconds
Overall accuracy: 85.9112%
Confusion   Matrix:
   a   b   <-- classified as
 286  71 |   a = 0
  21 275 |   b = 1

5.
weka.classifiers.functions.SMO -C 1.0 -L 0.001 -P 1.0E-12 -N 0 -V -1 -W 1 -K "weka.classifiers.functions.supportVector.PolyKernel -E 1.0 -C 250007" -calibrator "weka.classifiers.functions.Logistic -R 1.0E-8 -M -1 -num-decimal-places 4"
Time taken to build model: 0.59 seconds
Overall accuracy: 86.2175%
Confusion   Matrix:
   a   b   <-- classified as
 286  71 |   a = 0
  19 277 |   b = 1

Section B
1a.
In random forest model, I modified maxDepth from 0 to 10. Prediction accuracy increases from 86.6769% to 86.83%. It was because the decision node of decision tree from 0 to more nodes. In this case, with the growing number of nodes, the decision tree could have a better precision. Therefore, the accuracy is better.

1b. 
In Logistic regression, I modified ridge from 1.0E-8 to 1.0E-6. Running time decreases from 0.16 seconds to 0.05 seconds. It was because enlarging the ridge value, the ability to avoid overfitting decreases. Therefore, the processes do not need many time to fit the ridge value.

1c.
In MultilayerPerceptron model, I modified batchSize from 100 to 200. Running time decreases from 5.33 seconds to 5.22 seconds. It was because this program processes data per batch. With the twice of batch size, the total number of batches is half of the original. Although the processing time of each batch is increasing, the total time to process the whole data decreases.

1d.
In SVM model, I modified epsilon from 0.001 to 0.01. Running time decreases from 0.09 seconds to 0.05 seconds. It was because the error decreases, this model do not need such accuracy. Therefore, the processing time decreases because of large error.

1e.
In SMO model, I modified batchSize from 100 to 50. Running time decreases from 0.59 seconds to 0.22 seconds. It was because processing time within one batch decreases a lot, which bigger than the increasing time because of twice numbers of batches. Therefore, the processing time within one batch takes a major part in total time.

2.
The accuracy in random forest of my implementation was 85.60% , compared to wekaâ€™s 86.6769%. It was because that my implementation iteratively explore all the decision trees until the nodes cannot be partitioned. This method might cause mistakes in some situation, like the information gain is really small. Therefore, the enough exploration might be not very reasonable.

3.
In terms of running time and prediction accuracy, the best performing approach was SVM. Because the accuracies of five models above are all around 86% except for MultilayerPerceptron, we could choose one from the remaining four. Considering about the running time, 0.09 seconds is a kind of small running time. The reason why the SVM is so good is that this problem is binary classification problem, SVM only uses three support vectors to build this model, which means intuitively quick.